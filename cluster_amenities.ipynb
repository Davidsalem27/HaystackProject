{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import sys\n",
    "# \n",
    "# import bertopic\n",
    "# import tqdm\n",
    "# !{sys.executable} -m pip install stanza --upgrade\n",
    "# !{sys.executable} -m pip install torch --upgrade\n",
    "# \n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# \n",
    "# def plot_dendrogram(embeddings, labels):\n",
    "#     linked = linkage(embeddings, 'ward')\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     dendrogram(linked, labels=labels, leaf_rotation=90)\n",
    "#     plt.show()\n",
    "# \n",
    "# import pandas as pd\n",
    "# import ast\n",
    "# import re\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from tqdm import tqdm\n",
    "# \n",
    "# tqdm.pandas()  # Enables df.progress_apply\n",
    "# \n",
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "# \n",
    "# stop_words = set(stopwords.words(\"english\"))\n",
    "# stemmer = PorterStemmer()\n",
    "# \n",
    "# def normalize_text(text):\n",
    "#     words = re.findall(r'[a-z0-9]+', text.lower())\n",
    "#     return \" \".join(stemmer.stem(w) for w in words if w not in stop_words)\n",
    "# \n",
    "# \n",
    "# def extract_unique_amenities(df):\n",
    "#     unique = set()\n",
    "#     parsed_amenities = []\n",
    "#     \n",
    "#     for val in tqdm(df[\"amenities\"], desc=\"Parsing amenities\"):\n",
    "#         try:\n",
    "#             lst = ast.literal_eval(val) if isinstance(val, str) else []\n",
    "#             lst = [normalize_text(str(a)) for a in lst]\n",
    "#         except Exception:\n",
    "#             lst = []\n",
    "#         parsed_amenities.append(lst)\n",
    "#         unique.update(lst)\n",
    "#     print(len(unique))\n",
    "#     \n",
    "#     return list(unique), parsed_amenities\n",
    "# \n",
    "# from collections import defaultdict\n",
    "# \n",
    "# def cluster_amenities(unique_amenities, n_clusters=None):\n",
    "#     print(f\"Encoding {len(unique_amenities)} unique amenities...\")\n",
    "#     \n",
    "# \n",
    "#     model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# \n",
    "#     embeddings = list(tqdm(model.encode(unique_amenities, show_progress_bar=True), \n",
    "#                             total=len(unique_amenities), \n",
    "#                             desc=\"Encoding embeddings\"))\n",
    "#     \n",
    "#     if not n_clusters:\n",
    "#         n_clusters = max(2, int(len(unique_amenities) ** 0.5))\n",
    "#     \n",
    "#     print(f\"Clustering into {n_clusters} groups...\")\n",
    "#     clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "#     labels = clustering.fit_predict(embeddings)\n",
    "#     \n",
    "#     # Group amenities by cluster\n",
    "#     cluster_to_amenities = defaultdict(list)\n",
    "#     for amenity, label in zip(unique_amenities, labels):\n",
    "#         cluster_to_amenities[label].append(amenity)\n",
    "#     \n",
    "#     # Pick a representative name for each cluster\n",
    "#     cluster_names = {}\n",
    "#     for cluster, items in cluster_to_amenities.items():\n",
    "#         # Pick the shortest name (after sorting)\n",
    "#         rep_name = sorted(items, key=len)[0]\n",
    "#         # Clean name for column use\n",
    "#         rep_name = rep_name.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "#         cluster_names[cluster] = rep_name\n",
    "#     print(cluster_names)\n",
    "#     \n",
    "#     # Map amenity → cluster name\n",
    "#     amenity_to_cluster = {amenity: cluster_names[label] \n",
    "#                           for amenity, label in zip(unique_amenities, labels)}\n",
    "#     plot_dendrogram(embeddings, unique_amenities)\n",
    "# \n",
    "#     return amenity_to_cluster, cluster_to_amenities\n",
    "# \n",
    "# \n",
    "# def expand_amenities_semantic(df, amenity_to_cluster, parsed_amenities):\n",
    "#     cluster_names = set(amenity_to_cluster.values())\n",
    "#     \n",
    "#     # Initialize binary columns for each cluster name\n",
    "#     for cname in cluster_names:\n",
    "#         df[f\"amenity_{cname}\"] = 0\n",
    "#     \n",
    "#     for idx, lst in tqdm(enumerate(parsed_amenities), \n",
    "#                          total=len(parsed_amenities), \n",
    "#                          desc=\"Assigning amenities to clusters\"):\n",
    "#         cluster_ids = {amenity_to_cluster[a] for a in lst if a in amenity_to_cluster}\n",
    "#         for cname in cluster_ids:\n",
    "#             df.at[idx, f\"amenity_{cname}\"] = 1\n",
    "#     \n",
    "#     return df\n",
    "# \n",
    "# def process_airbnb_with_semantic_amenities(path, n_clusters=None):\n",
    "#     df = pd.read_csv(path)\n",
    "#     unique_amenities, parsed_amenities = extract_unique_amenities(df)\n",
    "#     print(type(unique_amenities))\n",
    "#     amenity_to_cluster, cluster_to_amenities = cluster_amenities(unique_amenities, n_clusters)\n",
    "#     df = expand_amenities_semantic(df, amenity_to_cluster, parsed_amenities)\n",
    "#     return df, cluster_to_amenities\n",
    "# \n",
    "# \n",
    "# if __name__ == \"__main__\":\n",
    "#     df, mapping = process_airbnb_with_semantic_amenities(r\"C:\\Users\\hodos\\Documents\\Uni\\Uni-Year-3\\Semester2\\Data\\cleaned_listings_amsterdam.csv\")\n",
    "#     # df.to_csv(\"cleaned_with_clusters.csv\", index=False)\n",
    "#     # print(\"Cluster mapping:\", mapping)\n",
    "# \n",
    "# df.head()\n"
   ],
   "id": "422f4773c78560ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T18:59:09.759359Z",
     "start_time": "2025-08-17T18:58:45.988130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "\n",
    "# --- NLP Setup ---\n",
    "try:\n",
    "    NLP = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"✓ spaCy NLP model loaded successfully.\")\n",
    "except OSError:\n",
    "    print(\"Error: spaCy model 'en_core_web_sm' not found.\")\n",
    "    print(\"Please run: python -m spacy download en_core_web_sm\")\n",
    "    exit()\n",
    "\n",
    "# --- Setup: Define Stop Words ---\n",
    "CUSTOM_STOP_WORDS = set(CountVectorizer(stop_words='english').get_stop_words()).union(['listing', 'available', 'extra cost', 'stay'])\n",
    "\n",
    "tqdm.pandas(desc=\"Processing DataFrame\")\n",
    "\n",
    "def clean_amenity_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "\n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # --- NEW: Handle the \"(word) allowed\" pattern ---\n",
    "    # This regex finds a word followed by \"allowed\" and merges them.\n",
    "    # e.g., \"pets allowed\" -> \"pets_allowed\"\n",
    "    text = re.sub(r'(\\w+)\\s+allowed\\b', r'\\1_allowed', text)\n",
    "\n",
    "    # 3. Remove other specific unwanted patterns\n",
    "    text = re.sub(r'\\brequest\\b', '', text)\n",
    "    text = re.sub(r'\\d+\\s+years?\\s+old', '', text)\n",
    "    text = re.sub(r'ages?\\s+\\d+', '', text)\n",
    "\n",
    "    # 4. Keep only lowercase English letters and underscores (for our new pattern)\n",
    "    text = re.sub(r'[^a-z\\s_]', '', text)\n",
    "    \n",
    "    # 5. Remove stop words\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in CUSTOM_STOP_WORDS]\n",
    "    text = ' '.join(filtered_words)\n",
    "\n",
    "    # 6. Consolidate multiple spaces and strip\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def _get_primary_object(phrase, nlp_model):\n",
    "    \"\"\"\n",
    "    Uses a \"right-most noun\" heuristic to find the most important object.\n",
    "    \"\"\"\n",
    "    doc = nlp_model(phrase)\n",
    "    try:\n",
    "        if not doc.noun_chunks:\n",
    "            return None\n",
    "        longest_chunk = max(doc.noun_chunks, key=lambda chunk: len(chunk.text.split()))\n",
    "        for token in reversed(longest_chunk):\n",
    "            if token.pos_ in ['NOUN', 'PROPN']:\n",
    "                return token.lemma_\n",
    "        return longest_chunk.root.lemma_\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def refine_and_group_amenities(candidates, nlp_model):\n",
    "    \"\"\"\n",
    "    Applies the hybrid refinement process.\n",
    "    \"\"\"\n",
    "    # --- Stage 1: Prune ---\n",
    "    print(f\"--> Pruning {len(candidates)} initial candidates...\")\n",
    "    candidates.sort(key=len, reverse=True)\n",
    "    temp_set = set(candidates)\n",
    "    for c in candidates:\n",
    "        if c in temp_set:\n",
    "            substrings = {other for other in temp_set if c != other and other in c}\n",
    "            temp_set.difference_update(substrings)\n",
    "    pruned_amenities = sorted(list(temp_set), key=len, reverse=True)\n",
    "    print(f\"--> After pruning, {len(pruned_amenities)} candidates remain.\")\n",
    "\n",
    "    # --- Stage 2: Grouping with Hybrid Logic ---\n",
    "    ORDERED_SYNONYM_GROUPS = [\n",
    "        # == High-Priority Overrides ==\n",
    "        ('hair_dryer',        ['hair dryer']),\n",
    "        ('dishwasher',        ['dishwasher']),\n",
    "        ('washing_machine',   ['washer', 'washing machine']),\n",
    "        ('clothes_dryer',     ['dryer', 'tumble dryer']),\n",
    "        ('security_camera',   ['security camera', 'camera']),\n",
    "        ('pets_allowed',      ['pets_allowed']), # Catch our transformed pattern\n",
    "        ('smoking_allowed',   ['smoking_allowed']),\n",
    "        ('drying_rack',       ['drying rack']),\n",
    "        ('conditioner',       ['conditioner']), # Override for the NLP weakness\n",
    "        \n",
    "        # == General Concepts and Synonyms ==\n",
    "        ('air_conditioning',  ['air conditioning', 'ac', 'aircon']),\n",
    "        ('closet',            ['closet', 'wardrobe', 'dresser']),\n",
    "        ('tv',                ['tv', 'hdtv', 'television', 'hbo', 'cable', 'netflix', 'hulu', 'amazon prime', 'disney']),\n",
    "        ('wifi',              ['wifi', 'wireless internet', 'internet', 'ethernet']),\n",
    "        ('kitchen',           ['kitchen']),\n",
    "        ('coffee_maker',      ['coffee', 'coffee maker', 'nespresso', 'keurig']),\n",
    "        ('dinnerware',        ['dinnerware']),\n",
    "        ('heating',           ['heating', 'heater']),\n",
    "        ('parking',           ['parking']),\n",
    "        ('pool',              ['pool']),\n",
    "        ('hot_tub',           ['hot tub', 'jacuzzi']),\n",
    "        ('bathtub',           ['bath', 'bathtub']),\n",
    "        ('patio_balcony',     ['patio', 'balcony']),\n",
    "        ('gym',               ['gym', 'fitness']),\n",
    "        ('first_aid_kit',     ['first aid']),\n",
    "        ('smoke_alarm',       ['smoke alarm', 'smoke detector']),\n",
    "        ('carbon_monoxide_alarm', ['carbon monoxide alarm', 'co alarm', 'carbon monoxide detector']),\n",
    "        ('fire_extinguisher', ['fire extinguisher']),\n",
    "        ('refrigerator',      ['refrigerator', 'fridge']),\n",
    "        ('microwave',         ['microwave']),\n",
    "        ('oven',              ['oven']),\n",
    "        ('stove',             ['stove', 'cooktop']),\n",
    "        ('books',             ['books']),\n",
    "        ('waterfront',        ['river', 'canal', 'waterfront']),\n",
    "        ('shampoo',           ['shampoo']),\n",
    "        ('body_soap',         ['soap', 'body soap'])\n",
    "    ]\n",
    "\n",
    "    groups = defaultdict(list)\n",
    "    \n",
    "    # First pass: Use the high-precision synonym list\n",
    "    amenities_to_process = set(pruned_amenities)\n",
    "    for standard_name, synonyms in tqdm(ORDERED_SYNONYM_GROUPS, desc=\"Grouping by Synonyms\"):\n",
    "        matched_this_pass = set()\n",
    "        for amenity in amenities_to_process:\n",
    "            for synonym in synonyms:\n",
    "                if re.search(r'\\b' + re.escape(synonym) + r'\\b', amenity):\n",
    "                    groups[standard_name].append(amenity)\n",
    "                    matched_this_pass.add(amenity)\n",
    "                    break \n",
    "        amenities_to_process.difference_update(matched_this_pass)\n",
    "\n",
    "    # Second pass: Use NLP as a fallback for everything else\n",
    "    print(f\"--> {len(amenities_to_process)} amenities remaining for NLP fallback grouping...\")\n",
    "    for amenity in tqdm(list(amenities_to_process), desc=\"Grouping by NLP\"):\n",
    "        primary_object = _get_primary_object(amenity, nlp_model)\n",
    "        if primary_object:\n",
    "            groups[primary_object].append(amenity)\n",
    "        else:\n",
    "            groups[amenity].append(amenity)\n",
    "\n",
    "    print(f\"--> Consolidated into {len(groups)} final amenity groups.\")\n",
    "    return groups\n",
    "\n",
    "def standardize_amenities_final(file_path, min_df_threshold=50):\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(file_path, engine='python', on_bad_lines='warn')\n",
    "\n",
    "    df['parsed_amenities'] = df['amenities'].progress_apply(\n",
    "        lambda s: ast.literal_eval(s.strip()) if isinstance(s, str) and s.strip().startswith('[') else []\n",
    "    )\n",
    "    \n",
    "    corpus = [clean_amenity_text(a) for l in df['parsed_amenities'] for a in l if clean_amenity_text(a)]\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 5), min_df=min_df_threshold)\n",
    "    vectorizer.fit(corpus)\n",
    "    initial_candidates = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "    final_amenity_groups = refine_and_group_amenities(initial_candidates, nlp)\n",
    "    print(final_amenity_groups)\n",
    "    df['cleaned_amenities_text'] = df['parsed_amenities'].progress_apply(\n",
    "        lambda lst: ' | '.join([clean_amenity_text(item) for item in lst])\n",
    "    )\n",
    "\n",
    "    for group_name, search_terms in tqdm(final_amenity_groups.items(), desc=\"Creating Columns\"):\n",
    "        column_name = f\"has_{group_name.replace(' ', '_')}\"\n",
    "        pattern = '|'.join([r'\\b' + re.escape(term) + r'\\b' for term in search_terms])\n",
    "        df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
    "\n",
    "    df = df.drop(columns=['parsed_amenities', 'cleaned_amenities_text'])\n",
    "    print(\"✓ Binary columns created successfully.\")\n",
    "    return df\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == '__main__':\n",
    "    csv_file_path = r\"C:\\Users\\hodos\\Documents\\Uni\\Uni-Year-3\\Semester2\\Data\\cleaned_listings_amsterdam.csv\"\n",
    "    MIN_DF_THRESHOLD = 100 \n",
    "\n",
    "    transformed_df = standardize_amenities_final(\n",
    "        csv_file_path, min_df_threshold=MIN_DF_THRESHOLD\n",
    "    )\n",
    "\n",
    "    if transformed_df is not None:\n",
    "        print(\"\\nTransformation complete. Here's a preview:\")\n",
    "        amenity_cols = sorted([col for col in transformed_df.columns if col.startswith('has_')])\n",
    "        display_cols = ['id', 'name'] + amenity_cols\n",
    "        \n",
    "        if len(display_cols) > 20:\n",
    "            print(f\"(Showing a subset of the {len(amenity_cols)} new amenity columns)\")\n",
    "            display_cols = display_cols[:20]\n",
    "\n",
    "        print(transformed_df[display_cols].head())"
   ],
   "id": "369fb0ad071de32c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ spaCy NLP model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: 100%|██████████| 10168/10168 [00:00<00:00, 15192.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Pruning 572 initial candidates...\n",
      "--> After pruning, 168 candidates remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping by Synonyms: 100%|██████████| 35/35 [00:00<00:00, 1589.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> 93 amenities remaining for NLP fallback grouping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping by NLP: 100%|██████████| 93/93 [00:00<00:00, 183.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Consolidated into 112 final amenity groups.\n",
      "defaultdict(<class 'list'>, {'hair_dryer': ['hair dryer'], 'dishwasher': ['dishwasher'], 'washing_machine': ['free washer building', 'free washer unit', 'paid washer'], 'clothes_dryer': ['free dryer building', 'paid dryer', 'free dryer unit'], 'pets_allowed': ['pets_allowed'], 'smoking_allowed': ['smoking_allowed'], 'drying_rack': ['drying rack clothing'], 'conditioner': ['conditioner'], 'air_conditioning': ['ac split type ductless', 'portable air conditioning', 'central air conditioning'], 'closet': ['clothing storage wardrobe', 'clothing storage closet dresser', 'clothing storage closet wardrobe dresser', 'clothing storage dresser', 'clothing storage walkin closet'], 'tv': ['inch hdtv amazon prime video', 'inch hdtv netflix', 'inch hdtv chromecast', 'hbo max netflix', 'hdtv standard cable', 'apple tv', 'netflix premium cable', 'chromecast netflix', 'netflix standard cable', 'disney', 'inch tv'], 'wifi': ['pocket wifi', 'fast wifi mbps', 'ethernet connection'], 'coffee_maker': ['coffee maker pourover coffee', 'machine nespresso', 'coffee maker drip coffee maker', 'maker drip coffee maker espresso', 'drip coffee maker espresso machine', 'coffee maker french press', 'coffee maker nespresso'], 'dinnerware': ['childrens dinnerware'], 'heating': ['radiant heating', 'central heating', 'portable heater'], 'parking': ['paid street parking premises', 'paid parking garage premises', 'free street parking', 'free parking premises', 'paid parking lot premises', 'paid parking premises'], 'hot_tub': ['hot tub'], 'bathtub': ['baby bath', 'bathtub'], 'patio_balcony': ['private patio balcony', 'shared patio balcony'], 'gym': ['gym'], 'smoke_alarm': ['smoke alarm'], 'carbon_monoxide_alarm': ['carbon monoxide alarm'], 'refrigerator': ['mini fridge', 'refrigerator'], 'microwave': ['microwave'], 'oven': ['double oven', 'stainless steel single oven', 'stainless steel oven'], 'stove': ['electric stove', 'stainless steel gas stove', 'induction stove'], 'books': ['books reading material', 'childrens books toys'], 'waterfront': ['river view', 'waterfront', 'canal view'], 'shampoo': ['shampoo'], 'body_soap': ['body soap'], 'mat': ['equipment free weights yoga mat'], 'view': ['courtyard view', 'garden view', 'city skyline view', 'park view'], 'playground': ['outdoor playground'], 'piano': ['piano'], 'cost': ['extra cost'], 'basic': ['cooking basics'], 'housekeeping': ['housekeeping'], 'siemen': ['siemens'], 'fan': ['ceiling fan', 'portable fans'], 'glass': ['wine glasses'], 'access': ['beach access beachfront', 'resort access', 'shared beach access', 'lake access'], 'entrance': ['private entrance'], 'lockbox': ['lockbox'], 'hammock': ['hammock'], 'hanger': ['hangers'], 'lounger': ['sun loungers'], 'kettle': ['hot water kettle'], 'elevator': ['elevator'], 'kit': ['aid kit'], 'maris': ['marie stella maris'], 'door': ['lock bedroom door'], 'games': ['board games'], 'workspace': ['dedicated workspace'], 'charcoal': ['bbq grill charcoal'], 'table': ['changing table', 'dining table'], 'net': ['mosquito net'], 'freezer': ['freezer'], 'kitchenette': ['kitchenette'], 'bluetooth': ['sound bluetooth aux', 'sonos bluetooth sound'], 'staff': ['building staff'], 'compactor': ['trash compactor'], 'furniture': ['outdoor furniture'], 'ritual': ['rituals'], 'lock': ['smart lock'], 'toaster': ['toaster'], 'chair': ['standalone high chair'], 'console': ['game console ps'], 'blender': ['blender'], 'barbecue': ['barbecue utensils'], 'boat': ['boat slip'], 'area': ['outdoor dining area'], 'charger': ['ev charger'], 'roomdarkening shades': ['roomdarkening shades'], 'room': ['private living room'], 'product': ['cleaning products'], 'aeg': ['aeg'], 'bidet': ['bidet'], 'fireplace': ['indoor fireplace woodburning'], 'dishes silverware': ['dishes silverware'], 'linen': ['bed linens'], 'player': ['record player'], 'maker': ['rice maker', 'bread maker'], 'blanket': ['extra pillows blankets'], 'backyard': ['private backyard fully fenced', 'shared backyard fully fenced'], 'essential': ['essentials'], 'property': ['exterior security cameras property'], 'sound': ['sonos sound'], 'guard': ['window guards'], 'breakfast': ['breakfast'], 'bosch': ['bosch'], 'gate': ['baby safety gates'], 'checkin': ['self checkin'], 'monitor': ['baby monitor'], 'sheet': ['baking sheet'], 'steel': ['smeg stainless steel'], 'greet': ['host greets'], 'playroom': ['childrens playroom'], 'extinguisher': ['extinguisher'], 'outlet': ['outlet covers'], 'bike': ['childrens bikes'], 'keypad': ['keypad'], 'luggage dropoff_allowed': ['luggage dropoff_allowed'], 'yoga': ['exercise equipment free weights yoga'], 'recommendation': ['babysitter recommendations'], 'term': ['long term stays_allowed'], 'dove': ['dove'], 'gel': ['shower gel'], 'iron': ['iron'], 'crib': ['pack playtravel crib'], 'pit': ['pit'], 'laundromat': ['laundromat nearby'], 'home': ['single level home']})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: 100%|██████████| 10168/10168 [00:02<00:00, 4552.72it/s]\n",
      "Creating Columns:  85%|████████▍ | 95/112 [00:10<00:01,  9.61it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  86%|████████▌ | 96/112 [00:10<00:01,  9.29it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  87%|████████▋ | 97/112 [00:10<00:01,  8.79it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  88%|████████▊ | 98/112 [00:10<00:01,  8.61it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  88%|████████▊ | 99/112 [00:10<00:01,  8.02it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  89%|████████▉ | 100/112 [00:10<00:01,  8.03it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  90%|█████████ | 101/112 [00:10<00:01,  8.00it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  91%|█████████ | 102/112 [00:10<00:01,  8.34it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  92%|█████████▏| 103/112 [00:11<00:01,  8.46it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  93%|█████████▎| 104/112 [00:11<00:00,  8.49it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  94%|█████████▍| 105/112 [00:11<00:00,  8.63it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  95%|█████████▍| 106/112 [00:11<00:00,  8.30it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  96%|█████████▋| 108/112 [00:11<00:00,  9.39it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  97%|█████████▋| 109/112 [00:11<00:00,  9.03it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  98%|█████████▊| 110/112 [00:11<00:00,  8.43it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  99%|█████████▉| 111/112 [00:12<00:00,  8.13it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_4512\\912353879.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns: 100%|██████████| 112/112 [00:12<00:00,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Binary columns created successfully.\n",
      "\n",
      "Transformation complete. Here's a preview:\n",
      "(Showing a subset of the 113 new amenity columns)\n",
      "      id                                               name  has_access  \\\n",
      "0  27886  Romantic, stylish B&B houseboat in canal district           1   \n",
      "1  28871                            Comfortable double room           0   \n",
      "2  29051                   Comfortable single / double room           0   \n",
      "3  44391    Quiet 2-bedroom Amsterdam city centre apartment           0   \n",
      "4  47061                   Charming apartment in old centre           0   \n",
      "\n",
      "   has_aeg  has_air_conditioning  has_area  has_availability  has_backyard  \\\n",
      "0        0                     0         1                 1             1   \n",
      "1        0                     0         0                 1             0   \n",
      "2        0                     0         0                 1             0   \n",
      "3        0                     0         0                 1             0   \n",
      "4        0                     0         0                 1             0   \n",
      "\n",
      "   has_barbecue  has_basic  has_bathtub  has_bidet  has_bike  has_blanket  \\\n",
      "0             0          0            0          0         0            1   \n",
      "1             0          0            0          0         0            0   \n",
      "2             0          0            0          0         0            0   \n",
      "3             0          1            0          0         0            1   \n",
      "4             0          1            0          0         0            0   \n",
      "\n",
      "   has_blender  has_bluetooth  has_boat  has_body_soap  has_books  has_bosch  \n",
      "0            0              0         1              1          1          0  \n",
      "1            0              0         0              0          0          0  \n",
      "2            0              0         0              0          0          0  \n",
      "3            0              0         0              0          0          0  \n",
      "4            0              0         0              1          1          0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T19:20:00.551883Z",
     "start_time": "2025-08-13T19:19:39.062091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "csv_file_path = r\"C:\\Users\\hodos\\Documents\\Uni\\Uni-Year-3\\Semester2\\Data\\cleaned_listings_amsterdam.csv\"\n",
    "MIN_DF_THRESHOLD = 100 \n",
    "\n",
    "\n",
    "transformed_df = standardize_amenities_final(\n",
    "    csv_file_path, min_df_threshold=MIN_DF_THRESHOLD\n",
    ")\n",
    "\n",
    "if transformed_df is not None:\n",
    "    print(\"\\nTransformation complete. Here's a preview:\")\n",
    "    amenity_cols = sorted([col for col in transformed_df.columns if col.startswith('has_')])\n",
    "    display_cols = ['id', 'name'] + amenity_cols\n",
    "    \n",
    "    if len(display_cols) > 20:\n",
    "        print(f\"(Showing a subset of the {len(amenity_cols)} new amenity columns)\")\n",
    "        display_cols = display_cols[:20]\n",
    "\n",
    "    print(transformed_df[display_cols].head())"
   ],
   "id": "e46e5c779b695b6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data from: C:\\Users\\hodos\\Documents\\Uni\\Uni-Year-3\\Semester2\\Data\\cleaned_listings_amsterdam.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: 100%|██████████| 10168/10168 [00:00<00:00, 15107.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Pruning 575 initial candidates...\n",
      "--> After pruning, 173 candidates remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping Amenities: 100%|██████████| 173/173 [00:00<00:00, 9970.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Consolidated into 125 final amenity groups.\n",
      "defaultdict(<class 'list'>, {'closet': ['clothing storage closet wardrobe', 'storage closet wardrobe dresser', 'clothing storage closet dresser', 'clothing storage walkin closet', 'clothing storage wardrobe', 'clothing storage dresser'], 'coffee_maker': ['coffee maker espresso machine', 'coffee maker pourover coffee', 'drip coffee maker espresso', 'coffee maker french press', 'coffee maker drip coffee', 'maker drip coffee maker', 'coffee maker nespresso', 'machine nespresso'], 'parking': ['paid parking garage premises', 'paid street parking premises', 'paid parking lot premises', 'free parking premises', 'paid parking premises', 'free street parking'], 'oven': ['stainless steel single oven', 'stainless steel oven', 'double oven'], 'air_conditioning': ['portable air conditioning', 'central air conditioning', 'ac split type ductless'], 'stove': ['stainless steel gas stove', 'induction stove', 'electric stove'], 'tv': ['hdtv amazon prime video', 'netflix standard cable', 'inch hdtv amazon prime', 'netflix premium cable', 'inch hdtv chromecast', 'hdtv standard cable', 'chromecast netflix', 'inch hdtv netflix', 'hbo max netflix', 'apple tv', 'inch tv', 'disney'], 'books': ['books reading material', 'childrens books toys'], 'patio_balcony': ['private patio balcony', 'shared patio balcony'], 'carbon_monoxide_alarm': ['carbon monoxide alarm'], 'washing_machine': ['free washer building', 'free washer unit', 'paid washer'], 'wifi': ['ethernet connection', 'fast wifi mbps', 'pocket wifi'], 'clothes_dryer': ['free dryer building', 'free dryer unit', 'paid dryer'], 'heating': ['radiant heating', 'portable heater', 'central heating'], 'refrigerator': ['refrigerator', 'mini fridge'], 'conditioner': ['conditioner'], 'smoke_alarm': ['smoke alarm'], 'waterfront': ['river view', 'canal view', 'waterfront'], 'dishwasher': ['dishwasher'], 'hair_dryer': ['hair dryer'], 'microwave': ['microwave'], 'bathtub': ['baby bath', 'bathtub'], 'body_soap': ['body soap'], 'shampoo': ['shampoo'], 'hot_tub': ['hot tub'], 'gym': ['gym'], 'exterior security cameras property': ['exterior security cameras property'], 'exercise equipment free weights': ['exercise equipment free weights'], 'private backyard fully fenced': ['private backyard fully fenced'], 'indoor fireplace woodburning': ['indoor fireplace woodburning'], 'shared backyard fully fenced': ['shared backyard fully fenced'], 'equipment free weights yoga': ['equipment free weights yoga'], 'babysitter recommendations': ['babysitter recommendations'], 'long term stays allowed': ['long term stays allowed'], 'luggage dropoff allowed': ['luggage dropoff allowed'], 'beach access beachfront': ['beach access beachfront'], 'extra pillows blankets': ['extra pillows blankets'], 'standalone high chair': ['standalone high chair'], 'free weights yoga mat': ['free weights yoga mat'], 'sonos bluetooth sound': ['sonos bluetooth sound'], 'childrens dinnerware': ['childrens dinnerware'], 'drying rack clothing': ['drying rack clothing'], 'pack playtravel crib': ['pack playtravel crib'], 'roomdarkening shades': ['roomdarkening shades'], 'smeg stainless steel': ['smeg stainless steel'], 'shared beach access': ['shared beach access'], 'sound bluetooth aux': ['sound bluetooth aux'], 'private living room': ['private living room'], 'outdoor dining area': ['outdoor dining area'], 'dedicated workspace': ['dedicated workspace'], 'bbq grill charcoal': ['bbq grill charcoal'], 'marie stella maris': ['marie stella maris'], 'outdoor playground': ['outdoor playground'], 'childrens playroom': ['childrens playroom'], 'lock bedroom door': ['lock bedroom door'], 'dishes silverware': ['dishes silverware'], 'single level home': ['single level home'], 'laundromat nearby': ['laundromat nearby'], 'city skyline view': ['city skyline view'], 'barbecue utensils': ['barbecue utensils'], 'baby safety gates': ['baby safety gates'], 'cleaning products': ['cleaning products'], 'outdoor furniture': ['outdoor furniture'], 'private entrance': ['private entrance'], 'hot water kettle': ['hot water kettle'], 'game console ps': ['game console ps'], 'trash compactor': ['trash compactor'], 'childrens bikes': ['childrens bikes'], 'smoking allowed': ['smoking allowed'], 'changing table': ['changing table'], 'courtyard view': ['courtyard view'], 'building staff': ['building staff'], 'cooking basics': ['cooking basics'], 'window guards': ['window guards'], 'outlet covers': ['outlet covers'], 'cleaning stay': ['cleaning stay'], 'record player': ['record player'], 'portable fans': ['portable fans'], 'resort access': ['resort access'], 'housekeeping': ['housekeeping'], 'baking sheet': ['baking sheet'], 'dining table': ['dining table'], 'mosquito net': ['mosquito net'], 'extinguisher': ['extinguisher'], 'wine glasses': ['wine glasses'], 'self checkin': ['self checkin'], 'sun loungers': ['sun loungers'], 'pets allowed': ['pets allowed'], 'baby monitor': ['baby monitor'], 'lake access': ['lake access'], 'bread maker': ['bread maker'], 'sonos sound': ['sonos sound'], 'kitchenette': ['kitchenette'], 'board games': ['board games'], 'host greets': ['host greets'], 'ceiling fan': ['ceiling fan'], 'garden view': ['garden view'], 'essentials': ['essentials'], 'smart lock': ['smart lock'], 'rice maker': ['rice maker'], 'shower gel': ['shower gel'], 'extra cost': ['extra cost'], 'bed linens': ['bed linens'], 'ev charger': ['ev charger'], 'boat slip': ['boat slip'], 'breakfast': ['breakfast'], 'park view': ['park view'], 'elevator': ['elevator'], 'blender': ['blender'], 'rituals': ['rituals'], 'aid kit': ['aid kit'], 'hangers': ['hangers'], 'toaster': ['toaster'], 'siemens': ['siemens'], 'hammock': ['hammock'], 'lockbox': ['lockbox'], 'freezer': ['freezer'], 'keypad': ['keypad'], 'piano': ['piano'], 'bidet': ['bidet'], 'bosch': ['bosch'], 'dove': ['dove'], 'iron': ['iron'], 'pit': ['pit'], 'aeg': ['aeg']})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: 100%|██████████| 10168/10168 [00:02<00:00, 4993.14it/s]\n",
      "Creating Columns:  76%|███████▌  | 95/125 [00:09<00:03,  9.20it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  77%|███████▋  | 96/125 [00:09<00:03,  8.84it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  78%|███████▊  | 98/125 [00:09<00:02, 10.74it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  80%|████████  | 100/125 [00:09<00:02, 10.35it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  82%|████████▏ | 102/125 [00:09<00:02, 10.74it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  83%|████████▎ | 104/125 [00:10<00:01, 10.70it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  85%|████████▍ | 106/125 [00:10<00:01,  9.99it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  86%|████████▋ | 108/125 [00:10<00:01,  9.84it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  88%|████████▊ | 110/125 [00:10<00:01,  9.91it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  89%|████████▉ | 111/125 [00:10<00:01,  9.89it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  90%|█████████ | 113/125 [00:11<00:01, 10.34it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  92%|█████████▏| 115/125 [00:11<00:01,  9.58it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  93%|█████████▎| 116/125 [00:11<00:00,  9.46it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  94%|█████████▍| 118/125 [00:11<00:00,  9.99it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  96%|█████████▌| 120/125 [00:11<00:00,  9.53it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  97%|█████████▋| 121/125 [00:11<00:00,  9.38it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  98%|█████████▊| 122/125 [00:12<00:00,  8.87it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  99%|█████████▉| 124/125 [00:12<00:00,  9.42it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns: 100%|██████████| 125/125 [00:12<00:00, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Binary columns created successfully.\n",
      "\n",
      "Transformation complete. Here's a preview:\n",
      "(Showing a subset of the 126 new amenity columns)\n",
      "      id                                               name  has_aeg  \\\n",
      "0  27886  Romantic, stylish B&B houseboat in canal district        0   \n",
      "1  28871                            Comfortable double room        0   \n",
      "2  29051                   Comfortable single / double room        0   \n",
      "3  44391    Quiet 2-bedroom Amsterdam city centre apartment        0   \n",
      "4  47061                   Charming apartment in old centre        0   \n",
      "\n",
      "   has_aid_kit  has_air_conditioning  has_availability  has_baby_monitor  \\\n",
      "0            0                     0                 1                 0   \n",
      "1            0                     0                 1                 0   \n",
      "2            0                     0                 1                 0   \n",
      "3            0                     0                 1                 0   \n",
      "4            1                     0                 1                 0   \n",
      "\n",
      "   has_baby_safety_gates  has_babysitter_recommendations  has_baking_sheet  \\\n",
      "0                      0                               0                 0   \n",
      "1                      0                               0                 0   \n",
      "2                      0                               0                 0   \n",
      "3                      0                               0                 0   \n",
      "4                      0                               0                 0   \n",
      "\n",
      "   has_barbecue_utensils  has_bathtub  has_bbq_grill_charcoal  \\\n",
      "0                      0            0                       0   \n",
      "1                      0            0                       0   \n",
      "2                      0            0                       0   \n",
      "3                      0            0                       0   \n",
      "4                      0            0                       0   \n",
      "\n",
      "   has_beach_access_beachfront  has_bed_linens  has_bidet  has_blender  \\\n",
      "0                            0               1          0            0   \n",
      "1                            0               1          0            0   \n",
      "2                            0               1          0            0   \n",
      "3                            0               1          0            0   \n",
      "4                            0               1          0            0   \n",
      "\n",
      "   has_board_games  has_boat_slip  has_body_soap  \n",
      "0                0              1              1  \n",
      "1                0              0              0  \n",
      "2                0              0              0  \n",
      "3                0              0              0  \n",
      "4                0              0              1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
