{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import sys\n",
    "# \n",
    "# import bertopic\n",
    "# import tqdm\n",
    "# !{sys.executable} -m pip install stanza --upgrade\n",
    "# !{sys.executable} -m pip install torch --upgrade\n",
    "# \n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# \n",
    "# def plot_dendrogram(embeddings, labels):\n",
    "#     linked = linkage(embeddings, 'ward')\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     dendrogram(linked, labels=labels, leaf_rotation=90)\n",
    "#     plt.show()\n",
    "# \n",
    "# import pandas as pd\n",
    "# import ast\n",
    "# import re\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from tqdm import tqdm\n",
    "# \n",
    "# tqdm.pandas()  # Enables df.progress_apply\n",
    "# \n",
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "# \n",
    "# stop_words = set(stopwords.words(\"english\"))\n",
    "# stemmer = PorterStemmer()\n",
    "# \n",
    "# def normalize_text(text):\n",
    "#     words = re.findall(r'[a-z0-9]+', text.lower())\n",
    "#     return \" \".join(stemmer.stem(w) for w in words if w not in stop_words)\n",
    "# \n",
    "# \n",
    "# def extract_unique_amenities(df):\n",
    "#     unique = set()\n",
    "#     parsed_amenities = []\n",
    "#     \n",
    "#     for val in tqdm(df[\"amenities\"], desc=\"Parsing amenities\"):\n",
    "#         try:\n",
    "#             lst = ast.literal_eval(val) if isinstance(val, str) else []\n",
    "#             lst = [normalize_text(str(a)) for a in lst]\n",
    "#         except Exception:\n",
    "#             lst = []\n",
    "#         parsed_amenities.append(lst)\n",
    "#         unique.update(lst)\n",
    "#     print(len(unique))\n",
    "#     \n",
    "#     return list(unique), parsed_amenities\n",
    "# \n",
    "# from collections import defaultdict\n",
    "# \n",
    "# def cluster_amenities(unique_amenities, n_clusters=None):\n",
    "#     print(f\"Encoding {len(unique_amenities)} unique amenities...\")\n",
    "#     \n",
    "# \n",
    "#     model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# \n",
    "#     embeddings = list(tqdm(model.encode(unique_amenities, show_progress_bar=True), \n",
    "#                             total=len(unique_amenities), \n",
    "#                             desc=\"Encoding embeddings\"))\n",
    "#     \n",
    "#     if not n_clusters:\n",
    "#         n_clusters = max(2, int(len(unique_amenities) ** 0.5))\n",
    "#     \n",
    "#     print(f\"Clustering into {n_clusters} groups...\")\n",
    "#     clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "#     labels = clustering.fit_predict(embeddings)\n",
    "#     \n",
    "#     # Group amenities by cluster\n",
    "#     cluster_to_amenities = defaultdict(list)\n",
    "#     for amenity, label in zip(unique_amenities, labels):\n",
    "#         cluster_to_amenities[label].append(amenity)\n",
    "#     \n",
    "#     # Pick a representative name for each cluster\n",
    "#     cluster_names = {}\n",
    "#     for cluster, items in cluster_to_amenities.items():\n",
    "#         # Pick the shortest name (after sorting)\n",
    "#         rep_name = sorted(items, key=len)[0]\n",
    "#         # Clean name for column use\n",
    "#         rep_name = rep_name.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "#         cluster_names[cluster] = rep_name\n",
    "#     print(cluster_names)\n",
    "#     \n",
    "#     # Map amenity → cluster name\n",
    "#     amenity_to_cluster = {amenity: cluster_names[label] \n",
    "#                           for amenity, label in zip(unique_amenities, labels)}\n",
    "#     plot_dendrogram(embeddings, unique_amenities)\n",
    "# \n",
    "#     return amenity_to_cluster, cluster_to_amenities\n",
    "# \n",
    "# \n",
    "# def expand_amenities_semantic(df, amenity_to_cluster, parsed_amenities):\n",
    "#     cluster_names = set(amenity_to_cluster.values())\n",
    "#     \n",
    "#     # Initialize binary columns for each cluster name\n",
    "#     for cname in cluster_names:\n",
    "#         df[f\"amenity_{cname}\"] = 0\n",
    "#     \n",
    "#     for idx, lst in tqdm(enumerate(parsed_amenities), \n",
    "#                          total=len(parsed_amenities), \n",
    "#                          desc=\"Assigning amenities to clusters\"):\n",
    "#         cluster_ids = {amenity_to_cluster[a] for a in lst if a in amenity_to_cluster}\n",
    "#         for cname in cluster_ids:\n",
    "#             df.at[idx, f\"amenity_{cname}\"] = 1\n",
    "#     \n",
    "#     return df\n",
    "# \n",
    "# def process_airbnb_with_semantic_amenities(path, n_clusters=None):\n",
    "#     df = pd.read_csv(path)\n",
    "#     unique_amenities, parsed_amenities = extract_unique_amenities(df)\n",
    "#     print(type(unique_amenities))\n",
    "#     amenity_to_cluster, cluster_to_amenities = cluster_amenities(unique_amenities, n_clusters)\n",
    "#     df = expand_amenities_semantic(df, amenity_to_cluster, parsed_amenities)\n",
    "#     return df, cluster_to_amenities\n",
    "# \n",
    "# \n",
    "# if __name__ == \"__main__\":\n",
    "#     df, mapping = process_airbnb_with_semantic_amenities(r\"C:\\Users\\hodos\\Documents\\Uni\\Uni-Year-3\\Semester2\\Data\\cleaned_listings_amsterdam.csv\")\n",
    "#     # df.to_csv(\"cleaned_with_clusters.csv\", index=False)\n",
    "#     # print(\"Cluster mapping:\", mapping)\n",
    "# \n",
    "# df.head()\n"
   ],
   "id": "422f4773c78560ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T19:11:36.158256Z",
     "start_time": "2025-08-13T19:11:36.129363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Import the standard English stop words list from scikit-learn\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Setup: Define Stop Words ---\n",
    "# For efficiency, create the full set of stop words once.\n",
    "# We add your custom words to the standard list.\n",
    "CUSTOM_STOP_WORDS = set(ENGLISH_STOP_WORDS).union(['listing', 'available'])\n",
    "\n",
    "# Initialize tqdm for pandas functions\n",
    "tqdm.pandas(desc=\"Processing DataFrame\")\n",
    "\n",
    "def clean_amenity_text(text):\n",
    "    \"\"\"\n",
    "    Cleans a single amenity string using a comprehensive multi-step process.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "\n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove specific unwanted patterns\n",
    "    text = re.sub(r'\\brequest\\b', '', text)\n",
    "    text = re.sub(r'\\d+\\s+years?\\s+old', '', text)\n",
    "    text = re.sub(r'ages?\\s+\\d+', '', text)\n",
    "\n",
    "    # 3. Keep only lowercase English letters (a-z) and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 4. Remove all stop words and custom words\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in CUSTOM_STOP_WORDS]\n",
    "    text = ' '.join(filtered_words)\n",
    "\n",
    "    # 5. Consolidate multiple spaces and strip leading/trailing whitespace\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def refine_and_group_amenities(candidates):\n",
    "    \"\"\"\n",
    "    Takes messy candidates and applies a two-stage refinement:\n",
    "    1. Prunes incomplete sub-phrases.\n",
    "    2. Groups the remaining amenities using a synonym dictionary.\n",
    "    \"\"\"\n",
    "    # --- Stage 1: Prune incomplete fragments (same as before) ---\n",
    "    print(f\"--> Pruning {len(candidates)} initial candidates...\")\n",
    "    candidates.sort(key=len, reverse=True)\n",
    "    pruned_amenities = []\n",
    "    temp_set = set(candidates)\n",
    "    # This logic keeps the longest phrase and discards any of its substrings\n",
    "    for candidate in candidates:\n",
    "        if candidate in temp_set:\n",
    "            # Create a copy of the set to iterate over while modifying the original\n",
    "            substrings_to_discard = {other for other in temp_set if candidate != other and other in candidate}\n",
    "            temp_set.difference_update(substrings_to_discard)\n",
    "            \n",
    "    pruned_amenities = sorted(list(temp_set), key=len, reverse=True)\n",
    "    print(f\"--> After pruning, {len(pruned_amenities)} candidates remain.\")\n",
    "\n",
    "    # --- Stage 2: Group by Synonyms ---\n",
    "    # This dictionary gives you full control over grouping.\n",
    "    # Key = final column name, Value = list of search terms/synonyms.\n",
    "    SYNONYM_GROUPS = {\n",
    "        'air_conditioning': ['air conditioning', 'ac', 'aircon'],\n",
    "        'closet':           ['closet', 'wardrobe','dresser'],\n",
    "        'tv':               ['tv', 'hdtv', 'television', 'hbo', 'cable', 'netflix', 'hulu', 'amazon prime', 'disney'],\n",
    "        'wifi':             ['wifi', 'wireless internet', 'internet', 'ethernet'],\n",
    "        'kitchen':          ['kitchen'],\n",
    "        'coffee_maker':     ['coffee', 'coffee maker', 'nespresso', 'keurig'],\n",
    "        'heating':          ['heating', 'heater'],\n",
    "        'parking':          ['parking'],\n",
    "        'pool':             ['pool'],\n",
    "        'hot_tub':          ['hot tub', 'jacuzzi'],\n",
    "        'bath':             ['bath', 'bathtub'],\n",
    "        'patio_balcony':    ['patio', 'balcony'],\n",
    "        'gym':              ['gym', 'fitness'],\n",
    "        'washer':           ['washer', 'washing machine'],\n",
    "        'dryer':            ['dryer'],\n",
    "        'first_aid':        ['first aid'],\n",
    "        'smoke_alarm':      ['smoke alarm', 'smoke detector'],\n",
    "        'carbon_monoxide_alarm': ['carbon monoxide alarm', 'carbon monoxide detector'],\n",
    "        'fire_extinguisher':['fire extinguisher'],\n",
    "        'refrigerator':     ['refrigerator', 'fridge'],\n",
    "        'microwave':        ['microwave'],\n",
    "        'oven':             ['oven'],\n",
    "        'stove':            ['stove'],\n",
    "        'books':            ['books'],\n",
    "        'river':            ['river', 'canal'],\n",
    "        'shampoo':          ['shampoo'],\n",
    "        'conditioner':      ['conditioner'],\n",
    "        'soap':             ['soap', 'body soap']\n",
    "    }\n",
    "\n",
    "    groups = defaultdict(list)\n",
    "    unassigned = []\n",
    "\n",
    "    for amenity in pruned_amenities:\n",
    "        was_grouped = False\n",
    "        # For each amenity, check which synonym group it belongs to\n",
    "        for standard_name, synonyms in SYNONYM_GROUPS.items():\n",
    "            # Use regex with word boundaries (\\b) to match whole words\n",
    "            for synonym in synonyms:\n",
    "                if re.search(r'\\b' + re.escape(synonym) + r'\\b', amenity):\n",
    "                    groups[standard_name].append(amenity)\n",
    "                    was_grouped = True\n",
    "                    break  # Stop checking other synonyms for this group\n",
    "            if was_grouped:\n",
    "                break  # Stop checking other groups for this amenity\n",
    "        \n",
    "        if not was_grouped:\n",
    "            unassigned.append(amenity)\n",
    "\n",
    "    # For amenities that weren't grouped, they become their own group\n",
    "    for amenity in unassigned:\n",
    "        groups[amenity].append(amenity)\n",
    "        \n",
    "    print(f\"--> Consolidated into {len(groups)} final amenity groups.\")\n",
    "    return groups\n",
    "\n",
    "def standardize_amenities_final(file_path, min_df_threshold=50):\n",
    "    print(f\"Attempting to load data from: {file_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, engine='python', on_bad_lines='warn')\n",
    "        print(\"✓ CSV file loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- Step 1: Parse Amenities ---\n",
    "    def parse_amenity_string(s):\n",
    "        if isinstance(s, str) and s.strip().startswith('['):\n",
    "            try: return ast.literal_eval(s.strip())\n",
    "            except: return []\n",
    "        return []\n",
    "    df['parsed_amenities'] = df['amenities'].progress_apply(parse_amenity_string)\n",
    "\n",
    "    # --- Step 2: Discover Candidates ---\n",
    "    corpus = [clean_amenity_text(a) for l in df['parsed_amenities'] for a in l if clean_amenity_text(a)]\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 4), stop_words='english', min_df=min_df_threshold)\n",
    "    vectorizer.fit(corpus)\n",
    "    initial_candidates = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "    # --- Step 3: Prune and Group Candidates using Synonym Logic ---\n",
    "    final_amenity_groups = refine_and_group_amenities(initial_candidates)\n",
    "    print(final_amenity_groups)\n",
    "    \n",
    "    # --- Step 4: Create Binary Columns from Final Groups ---\n",
    "    print(\"Creating binary columns from final groups...\")\n",
    "    df['cleaned_amenities_text'] = df['parsed_amenities'].progress_apply(\n",
    "        lambda lst: ' | '.join([clean_amenity_text(item) for item in lst])\n",
    "    )\n",
    "\n",
    "    for group_name, search_terms in tqdm(final_amenity_groups.items(), desc=\"Creating Columns\"):\n",
    "        # The column name is now the standardized key from the dictionary\n",
    "        column_name = f\"has_{group_name.replace(' ', '_')}\"\n",
    "        # Create a regex 'OR' pattern for all terms in the group\n",
    "        pattern = '|'.join([r'\\b' + re.escape(term) + r'\\b' for term in search_terms])\n",
    "        df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
    "\n",
    "    df = df.drop(columns=['parsed_amenities', 'cleaned_amenities_text'])\n",
    "    print(\"✓ Binary columns created successfully.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "   "
   ],
   "id": "369fb0ad071de32c",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T19:12:00.373339Z",
     "start_time": "2025-08-13T19:11:38.939607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "csv_file_path = r\"C:\\Users\\hodos\\Documents\\Uni\\Uni-Year-3\\Semester2\\Data\\cleaned_listings_amsterdam.csv\"\n",
    "MIN_DF_THRESHOLD = 100 \n",
    "\n",
    "\n",
    "transformed_df = standardize_amenities_final(\n",
    "    csv_file_path, min_df_threshold=MIN_DF_THRESHOLD\n",
    ")\n",
    "\n",
    "if transformed_df is not None:\n",
    "    print(\"\\nTransformation complete. Here's a preview:\")\n",
    "    amenity_cols = sorted([col for col in transformed_df.columns if col.startswith('has_')])\n",
    "    display_cols = ['id', 'name'] + amenity_cols\n",
    "    \n",
    "    if len(display_cols) > 20:\n",
    "        print(f\"(Showing a subset of the {len(amenity_cols)} new amenity columns)\")\n",
    "        display_cols = display_cols[:20]\n",
    "\n",
    "    print(transformed_df[display_cols].head())"
   ],
   "id": "e46e5c779b695b6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data from: C:\\Users\\hodos\\Documents\\Uni\\Uni-Year-3\\Semester2\\Data\\cleaned_listings_amsterdam.csv\n",
      "✓ CSV file loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: 100%|██████████| 10168/10168 [00:00<00:00, 15226.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Pruning 575 initial candidates...\n",
      "--> After pruning, 173 candidates remain.\n",
      "--> Consolidated into 125 final amenity groups.\n",
      "defaultdict(<class 'list'>, {'closet': ['clothing storage closet wardrobe', 'storage closet wardrobe dresser', 'clothing storage closet dresser', 'clothing storage walkin closet', 'clothing storage wardrobe', 'clothing storage dresser'], 'coffee_maker': ['coffee maker espresso machine', 'coffee maker pourover coffee', 'drip coffee maker espresso', 'coffee maker french press', 'coffee maker drip coffee', 'maker drip coffee maker', 'coffee maker nespresso', 'machine nespresso'], 'parking': ['paid parking garage premises', 'paid street parking premises', 'paid parking lot premises', 'free parking premises', 'paid parking premises', 'free street parking'], 'oven': ['stainless steel single oven', 'stainless steel oven', 'double oven'], 'air_conditioning': ['portable air conditioning', 'central air conditioning', 'ac split type ductless'], 'stove': ['stainless steel gas stove', 'induction stove', 'electric stove'], 'tv': ['hdtv amazon prime video', 'netflix standard cable', 'inch hdtv amazon prime', 'netflix premium cable', 'inch hdtv chromecast', 'hdtv standard cable', 'chromecast netflix', 'inch hdtv netflix', 'hbo max netflix', 'apple tv', 'inch tv', 'disney'], 'books': ['books reading material', 'childrens books toys'], 'patio_balcony': ['private patio balcony', 'shared patio balcony'], 'carbon_monoxide_alarm': ['carbon monoxide alarm'], 'washer': ['free washer building', 'free washer unit', 'paid washer'], 'wifi': ['ethernet connection', 'fast wifi mbps', 'pocket wifi'], 'dryer': ['free dryer building', 'free dryer unit', 'paid dryer', 'hair dryer'], 'heating': ['radiant heating', 'portable heater', 'central heating'], 'refrigerator': ['refrigerator', 'mini fridge'], 'conditioner': ['conditioner'], 'smoke_alarm': ['smoke alarm'], 'river': ['river view', 'canal view'], 'microwave': ['microwave'], 'bath': ['baby bath', 'bathtub'], 'soap': ['body soap'], 'shampoo': ['shampoo'], 'hot_tub': ['hot tub'], 'gym': ['gym'], 'exterior security cameras property': ['exterior security cameras property'], 'exercise equipment free weights': ['exercise equipment free weights'], 'private backyard fully fenced': ['private backyard fully fenced'], 'indoor fireplace woodburning': ['indoor fireplace woodburning'], 'shared backyard fully fenced': ['shared backyard fully fenced'], 'equipment free weights yoga': ['equipment free weights yoga'], 'babysitter recommendations': ['babysitter recommendations'], 'long term stays allowed': ['long term stays allowed'], 'luggage dropoff allowed': ['luggage dropoff allowed'], 'beach access beachfront': ['beach access beachfront'], 'extra pillows blankets': ['extra pillows blankets'], 'standalone high chair': ['standalone high chair'], 'free weights yoga mat': ['free weights yoga mat'], 'sonos bluetooth sound': ['sonos bluetooth sound'], 'childrens dinnerware': ['childrens dinnerware'], 'drying rack clothing': ['drying rack clothing'], 'pack playtravel crib': ['pack playtravel crib'], 'roomdarkening shades': ['roomdarkening shades'], 'smeg stainless steel': ['smeg stainless steel'], 'shared beach access': ['shared beach access'], 'sound bluetooth aux': ['sound bluetooth aux'], 'private living room': ['private living room'], 'outdoor dining area': ['outdoor dining area'], 'dedicated workspace': ['dedicated workspace'], 'bbq grill charcoal': ['bbq grill charcoal'], 'marie stella maris': ['marie stella maris'], 'outdoor playground': ['outdoor playground'], 'childrens playroom': ['childrens playroom'], 'lock bedroom door': ['lock bedroom door'], 'dishes silverware': ['dishes silverware'], 'single level home': ['single level home'], 'laundromat nearby': ['laundromat nearby'], 'city skyline view': ['city skyline view'], 'barbecue utensils': ['barbecue utensils'], 'baby safety gates': ['baby safety gates'], 'cleaning products': ['cleaning products'], 'outdoor furniture': ['outdoor furniture'], 'private entrance': ['private entrance'], 'hot water kettle': ['hot water kettle'], 'game console ps': ['game console ps'], 'trash compactor': ['trash compactor'], 'childrens bikes': ['childrens bikes'], 'smoking allowed': ['smoking allowed'], 'changing table': ['changing table'], 'courtyard view': ['courtyard view'], 'building staff': ['building staff'], 'cooking basics': ['cooking basics'], 'window guards': ['window guards'], 'outlet covers': ['outlet covers'], 'cleaning stay': ['cleaning stay'], 'record player': ['record player'], 'portable fans': ['portable fans'], 'resort access': ['resort access'], 'housekeeping': ['housekeeping'], 'baking sheet': ['baking sheet'], 'dining table': ['dining table'], 'mosquito net': ['mosquito net'], 'extinguisher': ['extinguisher'], 'wine glasses': ['wine glasses'], 'self checkin': ['self checkin'], 'sun loungers': ['sun loungers'], 'pets allowed': ['pets allowed'], 'baby monitor': ['baby monitor'], 'lake access': ['lake access'], 'bread maker': ['bread maker'], 'sonos sound': ['sonos sound'], 'kitchenette': ['kitchenette'], 'board games': ['board games'], 'host greets': ['host greets'], 'ceiling fan': ['ceiling fan'], 'garden view': ['garden view'], 'essentials': ['essentials'], 'smart lock': ['smart lock'], 'dishwasher': ['dishwasher'], 'rice maker': ['rice maker'], 'waterfront': ['waterfront'], 'shower gel': ['shower gel'], 'extra cost': ['extra cost'], 'bed linens': ['bed linens'], 'ev charger': ['ev charger'], 'boat slip': ['boat slip'], 'breakfast': ['breakfast'], 'park view': ['park view'], 'elevator': ['elevator'], 'blender': ['blender'], 'rituals': ['rituals'], 'aid kit': ['aid kit'], 'hangers': ['hangers'], 'toaster': ['toaster'], 'siemens': ['siemens'], 'hammock': ['hammock'], 'lockbox': ['lockbox'], 'freezer': ['freezer'], 'keypad': ['keypad'], 'piano': ['piano'], 'bidet': ['bidet'], 'bosch': ['bosch'], 'dove': ['dove'], 'iron': ['iron'], 'pit': ['pit'], 'aeg': ['aeg']})\n",
      "Creating binary columns from final groups...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: 100%|██████████| 10168/10168 [00:01<00:00, 5624.90it/s]\n",
      "Creating Columns:  76%|███████▌  | 95/125 [00:09<00:03,  7.73it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  78%|███████▊  | 97/125 [00:09<00:03,  9.03it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  79%|███████▉  | 99/125 [00:09<00:02,  9.73it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  80%|████████  | 100/125 [00:10<00:02,  9.52it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  82%|████████▏ | 102/125 [00:10<00:02,  9.26it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  82%|████████▏ | 103/125 [00:10<00:02,  9.40it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  83%|████████▎ | 104/125 [00:10<00:02,  9.26it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  84%|████████▍ | 105/125 [00:10<00:02,  9.00it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  85%|████████▍ | 106/125 [00:10<00:02,  8.35it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  86%|████████▌ | 107/125 [00:10<00:02,  8.13it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  86%|████████▋ | 108/125 [00:10<00:02,  7.83it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  88%|████████▊ | 110/125 [00:11<00:01,  8.36it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  89%|████████▉ | 111/125 [00:11<00:01,  8.24it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  90%|█████████ | 113/125 [00:11<00:01,  8.88it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  91%|█████████ | 114/125 [00:11<00:01,  8.72it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  92%|█████████▏| 115/125 [00:11<00:01,  8.37it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  93%|█████████▎| 116/125 [00:11<00:01,  8.40it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  94%|█████████▍| 118/125 [00:12<00:00,  8.68it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  95%|█████████▌| 119/125 [00:12<00:00,  8.44it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  96%|█████████▌| 120/125 [00:12<00:00,  8.24it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  97%|█████████▋| 121/125 [00:12<00:00,  8.35it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  98%|█████████▊| 122/125 [00:12<00:00,  8.00it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  98%|█████████▊| 123/125 [00:12<00:00,  8.21it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  99%|█████████▉| 124/125 [00:12<00:00,  7.80it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\2759465882.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns: 100%|██████████| 125/125 [00:13<00:00,  9.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Binary columns created successfully.\n",
      "\n",
      "Transformation complete. Here's a preview:\n",
      "(Showing a subset of the 126 new amenity columns)\n",
      "      id                                               name  has_aeg  \\\n",
      "0  27886  Romantic, stylish B&B houseboat in canal district        0   \n",
      "1  28871                            Comfortable double room        0   \n",
      "2  29051                   Comfortable single / double room        0   \n",
      "3  44391    Quiet 2-bedroom Amsterdam city centre apartment        0   \n",
      "4  47061                   Charming apartment in old centre        0   \n",
      "\n",
      "   has_aid_kit  has_air_conditioning  has_availability  has_baby_monitor  \\\n",
      "0            0                     0                 1                 0   \n",
      "1            0                     0                 1                 0   \n",
      "2            0                     0                 1                 0   \n",
      "3            0                     0                 1                 0   \n",
      "4            1                     0                 1                 0   \n",
      "\n",
      "   has_baby_safety_gates  has_babysitter_recommendations  has_baking_sheet  \\\n",
      "0                      0                               0                 0   \n",
      "1                      0                               0                 0   \n",
      "2                      0                               0                 0   \n",
      "3                      0                               0                 0   \n",
      "4                      0                               0                 0   \n",
      "\n",
      "   has_barbecue_utensils  has_bath  has_bbq_grill_charcoal  \\\n",
      "0                      0         0                       0   \n",
      "1                      0         0                       0   \n",
      "2                      0         0                       0   \n",
      "3                      0         0                       0   \n",
      "4                      0         0                       0   \n",
      "\n",
      "   has_beach_access_beachfront  has_bed_linens  has_bidet  has_blender  \\\n",
      "0                            0               1          0            0   \n",
      "1                            0               1          0            0   \n",
      "2                            0               1          0            0   \n",
      "3                            0               1          0            0   \n",
      "4                            0               1          0            0   \n",
      "\n",
      "   has_board_games  has_boat_slip  has_books  \n",
      "0                0              1          1  \n",
      "1                0              0          0  \n",
      "2                0              0          0  \n",
      "3                0              0          0  \n",
      "4                0              0          1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
