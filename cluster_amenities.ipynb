{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import sys\n",
    "# \n",
    "# import bertopic\n",
    "# import tqdm\n",
    "# !{sys.executable} -m pip install stanza --upgrade\n",
    "# !{sys.executable} -m pip install torch --upgrade\n",
    "# \n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# \n",
    "# def plot_dendrogram(embeddings, labels):\n",
    "#     linked = linkage(embeddings, 'ward')\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     dendrogram(linked, labels=labels, leaf_rotation=90)\n",
    "#     plt.show()\n",
    "# \n",
    "# import pandas as pd\n",
    "# import ast\n",
    "# import re\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from tqdm import tqdm\n",
    "# \n",
    "# tqdm.pandas()  # Enables df.progress_apply\n",
    "# \n",
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "# \n",
    "# stop_words = set(stopwords.words(\"english\"))\n",
    "# stemmer = PorterStemmer()\n",
    "# \n",
    "# def normalize_text(text):\n",
    "#     words = re.findall(r'[a-z0-9]+', text.lower())\n",
    "#     return \" \".join(stemmer.stem(w) for w in words if w not in stop_words)\n",
    "# \n",
    "# \n",
    "# def extract_unique_amenities(df):\n",
    "#     unique = set()\n",
    "#     parsed_amenities = []\n",
    "#     \n",
    "#     for val in tqdm(df[\"amenities\"], desc=\"Parsing amenities\"):\n",
    "#         try:\n",
    "#             lst = ast.literal_eval(val) if isinstance(val, str) else []\n",
    "#             lst = [normalize_text(str(a)) for a in lst]\n",
    "#         except Exception:\n",
    "#             lst = []\n",
    "#         parsed_amenities.append(lst)\n",
    "#         unique.update(lst)\n",
    "#     print(len(unique))\n",
    "#     \n",
    "#     return list(unique), parsed_amenities\n",
    "# \n",
    "# from collections import defaultdict\n",
    "# \n",
    "# def cluster_amenities(unique_amenities, n_clusters=None):\n",
    "#     print(f\"Encoding {len(unique_amenities)} unique amenities...\")\n",
    "#     \n",
    "# \n",
    "#     model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# \n",
    "#     embeddings = list(tqdm(model.encode(unique_amenities, show_progress_bar=True), \n",
    "#                             total=len(unique_amenities), \n",
    "#                             desc=\"Encoding embeddings\"))\n",
    "#     \n",
    "#     if not n_clusters:\n",
    "#         n_clusters = max(2, int(len(unique_amenities) ** 0.5))\n",
    "#     \n",
    "#     print(f\"Clustering into {n_clusters} groups...\")\n",
    "#     clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "#     labels = clustering.fit_predict(embeddings)\n",
    "#     \n",
    "#     # Group amenities by cluster\n",
    "#     cluster_to_amenities = defaultdict(list)\n",
    "#     for amenity, label in zip(unique_amenities, labels):\n",
    "#         cluster_to_amenities[label].append(amenity)\n",
    "#     \n",
    "#     # Pick a representative name for each cluster\n",
    "#     cluster_names = {}\n",
    "#     for cluster, items in cluster_to_amenities.items():\n",
    "#         # Pick the shortest name (after sorting)\n",
    "#         rep_name = sorted(items, key=len)[0]\n",
    "#         # Clean name for column use\n",
    "#         rep_name = rep_name.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "#         cluster_names[cluster] = rep_name\n",
    "#     print(cluster_names)\n",
    "#     \n",
    "#     # Map amenity → cluster name\n",
    "#     amenity_to_cluster = {amenity: cluster_names[label] \n",
    "#                           for amenity, label in zip(unique_amenities, labels)}\n",
    "#     plot_dendrogram(embeddings, unique_amenities)\n",
    "# \n",
    "#     return amenity_to_cluster, cluster_to_amenities\n",
    "# \n",
    "# \n",
    "# def expand_amenities_semantic(df, amenity_to_cluster, parsed_amenities):\n",
    "#     cluster_names = set(amenity_to_cluster.values())\n",
    "#     \n",
    "#     # Initialize binary columns for each cluster name\n",
    "#     for cname in cluster_names:\n",
    "#         df[f\"amenity_{cname}\"] = 0\n",
    "#     \n",
    "#     for idx, lst in tqdm(enumerate(parsed_amenities), \n",
    "#                          total=len(parsed_amenities), \n",
    "#                          desc=\"Assigning amenities to clusters\"):\n",
    "#         cluster_ids = {amenity_to_cluster[a] for a in lst if a in amenity_to_cluster}\n",
    "#         for cname in cluster_ids:\n",
    "#             df.at[idx, f\"amenity_{cname}\"] = 1\n",
    "#     \n",
    "#     return df\n",
    "# \n",
    "# def process_airbnb_with_semantic_amenities(path, n_clusters=None):\n",
    "#     df = pd.read_csv(path)\n",
    "#     unique_amenities, parsed_amenities = extract_unique_amenities(df)\n",
    "#     print(type(unique_amenities))\n",
    "#     amenity_to_cluster, cluster_to_amenities = cluster_amenities(unique_amenities, n_clusters)\n",
    "#     df = expand_amenities_semantic(df, amenity_to_cluster, parsed_amenities)\n",
    "#     return df, cluster_to_amenities\n",
    "# \n",
    "# \n",
    "# if __name__ == \"__main__\":\n",
    "#     df, mapping = process_airbnb_with_semantic_amenities(r\"C:\\Users\\hodos\\Documents\\Uni\\Uni-Year-3\\Semester2\\Data\\cleaned_listings_amsterdam.csv\")\n",
    "#     # df.to_csv(\"cleaned_with_clusters.csv\", index=False)\n",
    "#     # print(\"Cluster mapping:\", mapping)\n",
    "# \n",
    "# df.head()\n"
   ],
   "id": "422f4773c78560ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T19:19:36.154413Z",
     "start_time": "2025-08-13T19:19:36.053005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Setup: Define Stop Words ---\n",
    "CUSTOM_STOP_WORDS = set(CountVectorizer(stop_words='english').get_stop_words()).union(['listing', 'available'])\n",
    "\n",
    "tqdm.pandas(desc=\"Processing DataFrame\")\n",
    "\n",
    "def clean_amenity_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\brequest\\b', '', text)\n",
    "    text = re.sub(r'\\d+\\s+years?\\s+old', '', text)\n",
    "    text = re.sub(r'ages?\\s+\\d+', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in CUSTOM_STOP_WORDS]\n",
    "    text = ' '.join(filtered_words)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def refine_and_group_amenities(candidates):\n",
    "    \"\"\"\n",
    "    Applies a two-stage refinement using ordered priority for grouping.\n",
    "    1. Prunes incomplete sub-phrases.\n",
    "    2. Groups amenities using an ordered list to handle ambiguity.\n",
    "    \"\"\"\n",
    "    # --- Stage 1: Prune ---\n",
    "    print(f\"--> Pruning {len(candidates)} initial candidates...\")\n",
    "    candidates.sort(key=len, reverse=True)\n",
    "    temp_set = set(candidates)\n",
    "    for c in candidates:\n",
    "        if c in temp_set:\n",
    "            substrings = {other for other in temp_set if c != other and other in c}\n",
    "            temp_set.difference_update(substrings)\n",
    "    pruned_amenities = sorted(list(temp_set), key=len, reverse=True)\n",
    "    print(f\"--> After pruning, {len(pruned_amenities)} candidates remain.\")\n",
    "\n",
    "    # --- Stage 2: Grouping with Ordered Priority ---\n",
    "    # This is an ORDERED LIST of tuples. The order is CRITICAL.\n",
    "    # More specific, multi-word phrases MUST be listed BEFORE their shorter,\n",
    "    # more ambiguous counterparts.\n",
    "    ORDERED_SYNONYM_GROUPS = [\n",
    "        ('hair_dryer',        ['hair dryer']),\n",
    "        ('dishwasher',        ['dishwasher']),\n",
    "        ('washing_machine',   ['washer', 'washing machine']), # Catches clothes washer\n",
    "        ('clothes_dryer',     ['dryer', 'tumble dryer']),     # Catches clothes dryer\n",
    "        ('air_conditioning',  ['air conditioning', 'ac', 'aircon']),\n",
    "        ('closet',            ['closet', 'wardrobe', 'dresser']),\n",
    "        ('tv',                ['tv', 'hdtv', 'television', 'hbo', 'cable', 'netflix', 'hulu', 'amazon prime', 'disney']),\n",
    "        ('wifi',              ['wifi', 'wireless internet', 'internet', 'ethernet']),\n",
    "        ('kitchen',           ['kitchen']),\n",
    "        ('coffee_maker',      ['coffee', 'coffee maker', 'nespresso', 'keurig']),\n",
    "        ('heating',           ['heating', 'heater']),\n",
    "        ('parking',           ['parking']),\n",
    "        ('pool',              ['pool']),\n",
    "        ('hot_tub',           ['hot tub', 'jacuzzi']),\n",
    "        ('bathtub',           ['bath', 'bathtub', 'tub']),\n",
    "        ('patio_balcony',     ['patio', 'balcony']),\n",
    "        ('gym',               ['gym', 'fitness']),\n",
    "        ('first_aid_kit',     ['first aid']),\n",
    "        ('smoke_alarm',       ['smoke alarm', 'smoke detector']),\n",
    "        ('carbon_monoxide_alarm', ['carbon monoxide alarm', 'co alarm', 'carbon monoxide detector']),\n",
    "        ('fire_extinguisher', ['fire extinguisher']),\n",
    "        ('refrigerator',      ['refrigerator', 'fridge']),\n",
    "        ('microwave',         ['microwave']),\n",
    "        ('oven',              ['oven']),\n",
    "        ('stove',             ['stove', 'cooktop']),\n",
    "        ('books',             ['books']),\n",
    "        ('waterfront',        ['river', 'canal', 'waterfront']),\n",
    "        ('shampoo',           ['shampoo']),\n",
    "        ('conditioner',       ['conditioner']),\n",
    "        ('dish_soap', ['dish soap']),\n",
    "        ('shades', ['shades']),\n",
    "        ('body_soap',         ['soap', 'body soap'])\n",
    "    ]\n",
    "\n",
    "    groups = defaultdict(list)\n",
    "    unassigned = []\n",
    "\n",
    "    for amenity in tqdm(pruned_amenities, desc=\"Grouping Amenities\"):\n",
    "        was_grouped = False\n",
    "        # Iterate through the ORDERED list\n",
    "        for standard_name, synonyms in ORDERED_SYNONYM_GROUPS:\n",
    "            for synonym in synonyms:\n",
    "                if re.search(r'\\b' + re.escape(synonym) + r'\\b', amenity):\n",
    "                    groups[standard_name].append(amenity)\n",
    "                    was_grouped = True\n",
    "                    break  # Stop checking other synonyms for this group\n",
    "            if was_grouped:\n",
    "                break  # CRITICAL: Stop checking other groups for this amenity\n",
    "        \n",
    "        if not was_grouped:\n",
    "            unassigned.append(amenity)\n",
    "\n",
    "    # Handle any remaining unassigned amenities\n",
    "    for amenity in unassigned:\n",
    "        groups[amenity].append(amenity)\n",
    "\n",
    "    print(f\"--> Consolidated into {len(groups)} final amenity groups.\")\n",
    "    return groups\n",
    "\n",
    "def standardize_amenities_final(file_path, min_df_threshold=50):\n",
    "    print(f\"Attempting to load data from: {file_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, engine='python', on_bad_lines='warn')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    df['parsed_amenities'] = df['amenities'].progress_apply(\n",
    "        lambda s: ast.literal_eval(s.strip()) if isinstance(s, str) and s.strip().startswith('[') else []\n",
    "    )\n",
    "    \n",
    "    corpus = [clean_amenity_text(a) for l in df['parsed_amenities'] for a in l if clean_amenity_text(a)]\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 4), min_df=min_df_threshold)\n",
    "    vectorizer.fit(corpus)\n",
    "    initial_candidates = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "    final_amenity_groups = refine_and_group_amenities(initial_candidates)\n",
    "    print(final_amenity_groups)\n",
    "    \n",
    "    df['cleaned_amenities_text'] = df['parsed_amenities'].progress_apply(\n",
    "        lambda lst: ' | '.join([clean_amenity_text(item) for item in lst])\n",
    "    )\n",
    "\n",
    "    for group_name, search_terms in tqdm(final_amenity_groups.items(), desc=\"Creating Columns\"):\n",
    "        column_name = f\"has_{group_name.replace(' ', '_')}\"\n",
    "        pattern = '|'.join([r'\\b' + re.escape(term) + r'\\b' for term in search_terms])\n",
    "        df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
    "\n",
    "    df = df.drop(columns=['parsed_amenities', 'cleaned_amenities_text'])\n",
    "    print(\"✓ Binary columns created successfully.\")\n",
    "    return df\n",
    "\n",
    "\n"
   ],
   "id": "369fb0ad071de32c",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T19:20:00.551883Z",
     "start_time": "2025-08-13T19:19:39.062091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "csv_file_path = r\"C:\\Users\\hodos\\Documents\\Uni\\Uni-Year-3\\Semester2\\Data\\cleaned_listings_amsterdam.csv\"\n",
    "MIN_DF_THRESHOLD = 100 \n",
    "\n",
    "\n",
    "transformed_df = standardize_amenities_final(\n",
    "    csv_file_path, min_df_threshold=MIN_DF_THRESHOLD\n",
    ")\n",
    "\n",
    "if transformed_df is not None:\n",
    "    print(\"\\nTransformation complete. Here's a preview:\")\n",
    "    amenity_cols = sorted([col for col in transformed_df.columns if col.startswith('has_')])\n",
    "    display_cols = ['id', 'name'] + amenity_cols\n",
    "    \n",
    "    if len(display_cols) > 20:\n",
    "        print(f\"(Showing a subset of the {len(amenity_cols)} new amenity columns)\")\n",
    "        display_cols = display_cols[:20]\n",
    "\n",
    "    print(transformed_df[display_cols].head())"
   ],
   "id": "e46e5c779b695b6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data from: C:\\Users\\hodos\\Documents\\Uni\\Uni-Year-3\\Semester2\\Data\\cleaned_listings_amsterdam.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: 100%|██████████| 10168/10168 [00:00<00:00, 15107.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Pruning 575 initial candidates...\n",
      "--> After pruning, 173 candidates remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping Amenities: 100%|██████████| 173/173 [00:00<00:00, 9970.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Consolidated into 125 final amenity groups.\n",
      "defaultdict(<class 'list'>, {'closet': ['clothing storage closet wardrobe', 'storage closet wardrobe dresser', 'clothing storage closet dresser', 'clothing storage walkin closet', 'clothing storage wardrobe', 'clothing storage dresser'], 'coffee_maker': ['coffee maker espresso machine', 'coffee maker pourover coffee', 'drip coffee maker espresso', 'coffee maker french press', 'coffee maker drip coffee', 'maker drip coffee maker', 'coffee maker nespresso', 'machine nespresso'], 'parking': ['paid parking garage premises', 'paid street parking premises', 'paid parking lot premises', 'free parking premises', 'paid parking premises', 'free street parking'], 'oven': ['stainless steel single oven', 'stainless steel oven', 'double oven'], 'air_conditioning': ['portable air conditioning', 'central air conditioning', 'ac split type ductless'], 'stove': ['stainless steel gas stove', 'induction stove', 'electric stove'], 'tv': ['hdtv amazon prime video', 'netflix standard cable', 'inch hdtv amazon prime', 'netflix premium cable', 'inch hdtv chromecast', 'hdtv standard cable', 'chromecast netflix', 'inch hdtv netflix', 'hbo max netflix', 'apple tv', 'inch tv', 'disney'], 'books': ['books reading material', 'childrens books toys'], 'patio_balcony': ['private patio balcony', 'shared patio balcony'], 'carbon_monoxide_alarm': ['carbon monoxide alarm'], 'washing_machine': ['free washer building', 'free washer unit', 'paid washer'], 'wifi': ['ethernet connection', 'fast wifi mbps', 'pocket wifi'], 'clothes_dryer': ['free dryer building', 'free dryer unit', 'paid dryer'], 'heating': ['radiant heating', 'portable heater', 'central heating'], 'refrigerator': ['refrigerator', 'mini fridge'], 'conditioner': ['conditioner'], 'smoke_alarm': ['smoke alarm'], 'waterfront': ['river view', 'canal view', 'waterfront'], 'dishwasher': ['dishwasher'], 'hair_dryer': ['hair dryer'], 'microwave': ['microwave'], 'bathtub': ['baby bath', 'bathtub'], 'body_soap': ['body soap'], 'shampoo': ['shampoo'], 'hot_tub': ['hot tub'], 'gym': ['gym'], 'exterior security cameras property': ['exterior security cameras property'], 'exercise equipment free weights': ['exercise equipment free weights'], 'private backyard fully fenced': ['private backyard fully fenced'], 'indoor fireplace woodburning': ['indoor fireplace woodburning'], 'shared backyard fully fenced': ['shared backyard fully fenced'], 'equipment free weights yoga': ['equipment free weights yoga'], 'babysitter recommendations': ['babysitter recommendations'], 'long term stays allowed': ['long term stays allowed'], 'luggage dropoff allowed': ['luggage dropoff allowed'], 'beach access beachfront': ['beach access beachfront'], 'extra pillows blankets': ['extra pillows blankets'], 'standalone high chair': ['standalone high chair'], 'free weights yoga mat': ['free weights yoga mat'], 'sonos bluetooth sound': ['sonos bluetooth sound'], 'childrens dinnerware': ['childrens dinnerware'], 'drying rack clothing': ['drying rack clothing'], 'pack playtravel crib': ['pack playtravel crib'], 'roomdarkening shades': ['roomdarkening shades'], 'smeg stainless steel': ['smeg stainless steel'], 'shared beach access': ['shared beach access'], 'sound bluetooth aux': ['sound bluetooth aux'], 'private living room': ['private living room'], 'outdoor dining area': ['outdoor dining area'], 'dedicated workspace': ['dedicated workspace'], 'bbq grill charcoal': ['bbq grill charcoal'], 'marie stella maris': ['marie stella maris'], 'outdoor playground': ['outdoor playground'], 'childrens playroom': ['childrens playroom'], 'lock bedroom door': ['lock bedroom door'], 'dishes silverware': ['dishes silverware'], 'single level home': ['single level home'], 'laundromat nearby': ['laundromat nearby'], 'city skyline view': ['city skyline view'], 'barbecue utensils': ['barbecue utensils'], 'baby safety gates': ['baby safety gates'], 'cleaning products': ['cleaning products'], 'outdoor furniture': ['outdoor furniture'], 'private entrance': ['private entrance'], 'hot water kettle': ['hot water kettle'], 'game console ps': ['game console ps'], 'trash compactor': ['trash compactor'], 'childrens bikes': ['childrens bikes'], 'smoking allowed': ['smoking allowed'], 'changing table': ['changing table'], 'courtyard view': ['courtyard view'], 'building staff': ['building staff'], 'cooking basics': ['cooking basics'], 'window guards': ['window guards'], 'outlet covers': ['outlet covers'], 'cleaning stay': ['cleaning stay'], 'record player': ['record player'], 'portable fans': ['portable fans'], 'resort access': ['resort access'], 'housekeeping': ['housekeeping'], 'baking sheet': ['baking sheet'], 'dining table': ['dining table'], 'mosquito net': ['mosquito net'], 'extinguisher': ['extinguisher'], 'wine glasses': ['wine glasses'], 'self checkin': ['self checkin'], 'sun loungers': ['sun loungers'], 'pets allowed': ['pets allowed'], 'baby monitor': ['baby monitor'], 'lake access': ['lake access'], 'bread maker': ['bread maker'], 'sonos sound': ['sonos sound'], 'kitchenette': ['kitchenette'], 'board games': ['board games'], 'host greets': ['host greets'], 'ceiling fan': ['ceiling fan'], 'garden view': ['garden view'], 'essentials': ['essentials'], 'smart lock': ['smart lock'], 'rice maker': ['rice maker'], 'shower gel': ['shower gel'], 'extra cost': ['extra cost'], 'bed linens': ['bed linens'], 'ev charger': ['ev charger'], 'boat slip': ['boat slip'], 'breakfast': ['breakfast'], 'park view': ['park view'], 'elevator': ['elevator'], 'blender': ['blender'], 'rituals': ['rituals'], 'aid kit': ['aid kit'], 'hangers': ['hangers'], 'toaster': ['toaster'], 'siemens': ['siemens'], 'hammock': ['hammock'], 'lockbox': ['lockbox'], 'freezer': ['freezer'], 'keypad': ['keypad'], 'piano': ['piano'], 'bidet': ['bidet'], 'bosch': ['bosch'], 'dove': ['dove'], 'iron': ['iron'], 'pit': ['pit'], 'aeg': ['aeg']})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: 100%|██████████| 10168/10168 [00:02<00:00, 4993.14it/s]\n",
      "Creating Columns:  76%|███████▌  | 95/125 [00:09<00:03,  9.20it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  77%|███████▋  | 96/125 [00:09<00:03,  8.84it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  78%|███████▊  | 98/125 [00:09<00:02, 10.74it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  80%|████████  | 100/125 [00:09<00:02, 10.35it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  82%|████████▏ | 102/125 [00:09<00:02, 10.74it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  83%|████████▎ | 104/125 [00:10<00:01, 10.70it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  85%|████████▍ | 106/125 [00:10<00:01,  9.99it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  86%|████████▋ | 108/125 [00:10<00:01,  9.84it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  88%|████████▊ | 110/125 [00:10<00:01,  9.91it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  89%|████████▉ | 111/125 [00:10<00:01,  9.89it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  90%|█████████ | 113/125 [00:11<00:01, 10.34it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  92%|█████████▏| 115/125 [00:11<00:01,  9.58it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  93%|█████████▎| 116/125 [00:11<00:00,  9.46it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  94%|█████████▍| 118/125 [00:11<00:00,  9.99it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  96%|█████████▌| 120/125 [00:11<00:00,  9.53it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  97%|█████████▋| 121/125 [00:11<00:00,  9.38it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  98%|█████████▊| 122/125 [00:12<00:00,  8.87it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns:  99%|█████████▉| 124/125 [00:12<00:00,  9.42it/s]C:\\Users\\hodos\\AppData\\Local\\Temp\\ipykernel_8152\\1787245129.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df['cleaned_amenities_text'].str.contains(pattern, na=False).astype(int)\n",
      "Creating Columns: 100%|██████████| 125/125 [00:12<00:00, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Binary columns created successfully.\n",
      "\n",
      "Transformation complete. Here's a preview:\n",
      "(Showing a subset of the 126 new amenity columns)\n",
      "      id                                               name  has_aeg  \\\n",
      "0  27886  Romantic, stylish B&B houseboat in canal district        0   \n",
      "1  28871                            Comfortable double room        0   \n",
      "2  29051                   Comfortable single / double room        0   \n",
      "3  44391    Quiet 2-bedroom Amsterdam city centre apartment        0   \n",
      "4  47061                   Charming apartment in old centre        0   \n",
      "\n",
      "   has_aid_kit  has_air_conditioning  has_availability  has_baby_monitor  \\\n",
      "0            0                     0                 1                 0   \n",
      "1            0                     0                 1                 0   \n",
      "2            0                     0                 1                 0   \n",
      "3            0                     0                 1                 0   \n",
      "4            1                     0                 1                 0   \n",
      "\n",
      "   has_baby_safety_gates  has_babysitter_recommendations  has_baking_sheet  \\\n",
      "0                      0                               0                 0   \n",
      "1                      0                               0                 0   \n",
      "2                      0                               0                 0   \n",
      "3                      0                               0                 0   \n",
      "4                      0                               0                 0   \n",
      "\n",
      "   has_barbecue_utensils  has_bathtub  has_bbq_grill_charcoal  \\\n",
      "0                      0            0                       0   \n",
      "1                      0            0                       0   \n",
      "2                      0            0                       0   \n",
      "3                      0            0                       0   \n",
      "4                      0            0                       0   \n",
      "\n",
      "   has_beach_access_beachfront  has_bed_linens  has_bidet  has_blender  \\\n",
      "0                            0               1          0            0   \n",
      "1                            0               1          0            0   \n",
      "2                            0               1          0            0   \n",
      "3                            0               1          0            0   \n",
      "4                            0               1          0            0   \n",
      "\n",
      "   has_board_games  has_boat_slip  has_body_soap  \n",
      "0                0              1              1  \n",
      "1                0              0              0  \n",
      "2                0              0              0  \n",
      "3                0              0              0  \n",
      "4                0              0              1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
