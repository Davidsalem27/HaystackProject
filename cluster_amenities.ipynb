{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os # Import the os module to handle file paths\n",
    "\n",
    "# --- Library Imports for Flair ---\n",
    "try:\n",
    "    from flair.data import Sentence\n",
    "    from flair.models import SequenceTagger\n",
    "    FLAIR_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: Flair library not found. Brand removal will be skipped.\")\n",
    "    FLAIR_AVAILABLE = False\n",
    "\n",
    "# --- NLP Setup ---\n",
    "try:\n",
    "    NLP = spacy.load(\"en_core_web_md\")\n",
    "    print(\"✓ spaCy NLP model (md) with word vectors loaded successfully.\")\n",
    "except OSError:\n",
    "    print(\"Error: spaCy model 'en_core_web_md' not found. Please run: python -m spacy download en_core_web_md\")\n",
    "    exit()\n",
    "\n",
    "# --- Setup: Define Stop Words ---\n",
    "CUSTOM_STOP_WORDS = set(CountVectorizer(stop_words='english').get_stop_words()).union(['listing', 'extra', 'cost', 'included', 'private'])\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_amenity_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in CUSTOM_STOP_WORDS]\n",
    "    text = ' '.join(filtered_words)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# --- Flair Brand Remover Class ---\n",
    "class FlairBrandRemover:\n",
    "    def __init__(self):\n",
    "        self.tagger = None\n",
    "        if FLAIR_AVAILABLE:\n",
    "            try:\n",
    "                self.tagger = SequenceTagger.load('ner-large')\n",
    "                print(\"✓ Flair NER model loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load Flair NER model. Error: {e}\")\n",
    "                self.tagger = None\n",
    "    \n",
    "    def remove_brands(self, text):\n",
    "        if not self.tagger or not text: return text\n",
    "        sentence = Sentence(text)\n",
    "        self.tagger.predict(sentence)\n",
    "        brands_to_remove = {entity.text.lower() for entity in sentence.get_spans('ner') if entity.tag in ['ORG', 'PER', 'MISC']}\n",
    "        if not brands_to_remove: return text\n",
    "        words = text.split()\n",
    "        non_brand_words = [word for word in words if word.lower() not in brands_to_remove]\n",
    "        return ' '.join(non_brand_words)\n",
    "\n",
    "# --- Semantic Classifier Class ---\n",
    "class AmenityClassifier:\n",
    "    def __init__(self, category_definitions):\n",
    "        self.category_vectors = {}\n",
    "        print(\"Calculating semantic vectors for categories...\")\n",
    "        for category, examples in tqdm(category_definitions.items(), desc=\"Vectorizing Categories\"):\n",
    "            example_vectors = [NLP(example).vector for example in examples if NLP(example).has_vector]\n",
    "            if example_vectors:\n",
    "                self.category_vectors[category] = np.mean(example_vectors, axis=0)\n",
    "\n",
    "    def _cosine_similarity(self, vec1, vec2):\n",
    "        if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0: return 0\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    def predict(self, amenity_text, threshold=0.4):\n",
    "        if not amenity_text or not NLP(amenity_text).has_vector: return None\n",
    "        amenity_vector = NLP(amenity_text).vector\n",
    "        best_category, max_similarity = None, -1\n",
    "        for category, category_vector in self.category_vectors.items():\n",
    "            similarity = self._cosine_similarity(amenity_vector, category_vector)\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity, best_category = similarity, category\n",
    "        if max_similarity > threshold:\n",
    "            return best_category\n",
    "        return None\n",
    "\n",
    "# --- Main Logic ---\n",
    "AMENITY_CATEGORIES = {\n",
    "    'safety_core': ['smoke alarm', 'carbon monoxide alarm', 'fire extinguisher', 'lock'],\n",
    "    'safety_extra': ['first aid', 'lockbox', 'keypad', 'safe', 'guards'],\n",
    "    'kitchen_core': ['kitchen', 'refrigerator', 'fridge', 'oven', 'stove', 'cooktop', 'microwave', 'kettle', 'hot water kettle', 'freezer'],\n",
    "    'kitchen_extra': ['dishwasher', 'blender', 'baking', 'toaster', 'kitchenette', 'rice maker'],\n",
    "    'laundry_core': ['washer', 'dryer', 'washing machine', 'tumble dryer', 'iron', 'drying rack'],\n",
    "    'laundry_extra': ['laundromat'],\n",
    "    'air_conditioner_core': ['air conditioning', 'ac', 'aircon'],\n",
    "    'air_conditioner_extra': ['fan'],\n",
    "    'heater_core': ['heating'],\n",
    "    'heater_extra': ['heater', 'fireplace'],\n",
    "    'hygiene_core': ['shampoo', 'soap', 'towel', 'shower', 'hot water', 'essentials', 'conditioner', 'body soap'],\n",
    "    'hygiene_extra': ['bathtub', 'bath', 'bidet'],\n",
    "    'wellness_core': ['sauna', 'hot tub', 'jacuzzi', 'spa'],\n",
    "    'storage_core': ['closet', 'wardrobe', 'dresser'],\n",
    "    'storage_extra': ['hangers'],\n",
    "    'entertainment_core': ['tv', 'hdtv', 'television'],\n",
    "    'entertainment_extra': ['hbo', 'cable', 'netflix', 'hulu', 'amazon prime', 'disney', 'books', 'games', 'console','record player', 'piano'],\n",
    "    'internet_core': ['wifi', 'internet', 'wireless internet', 'ethernet'],\n",
    "    'parking_core': ['parking', 'garage'],\n",
    "    'parking_extra': ['ev charger'],\n",
    "    'exercise_core': ['gym','fitness', 'exercise', 'weights', 'treadmill', 'yoga mat', 'ping pong table'],\n",
    "    'exercise_extra': ['climbing wall'],\n",
    "    'outdoors_core': ['patio', 'balcony', 'pool'],\n",
    "    'outdoors_extra': ['grill', 'bbq', 'lounger', 'lounge', 'fire pit', 'firepit'],\n",
    "    'children_care_core': ['crib', 'high chair', 'children', 'baby', 'changing table'],\n",
    "    'children_care_extra': ['childrens dinnerware', 'babysitter'],\n",
    "    'cleaning_core': ['cleaning'],\n",
    "    'cleaning_extra': ['housekeeping'],\n",
    "    'pets_allowed': ['pets allowed'],\n",
    "    'smoking_allowed': ['smoking allowed'],\n",
    "    'coffee_maker_core': ['coffee maker', 'coffee', 'nespresso', 'keurig'],\n",
    "    'cooking_basics_core': ['cooking basics', 'dinnerware', 'table'],\n",
    "    'sound_system_core': ['sound system', 'sound', 'speakers'],\n",
    "    'accessibility_core': ['elevator', 'lift', 'wheelchair ramp', 'ground floor'],\n",
    "    'view_core': ['view', 'waterfront'],\n",
    "    'bedroom_core': ['bed', 'pillow', 'linens', 'blanket', 'duvet', 'sheet'],\n",
    "    'attractions_nearby_core': ['restaurant', 'coffee shop', 'lake', 'beach', 'river', 'center', 'golf', 'bowling', 'resort'],\n",
    "    'attractions_nearby_extra': ['museum'],\n",
    "    'hospitality_core':['self check-in'],\n",
    "    'hospitality_extra':['host greets', 'building staff']\n",
    "}\n",
    "\n",
    "CATEGORY_REPRESENTATIVES = {\n",
    "    'exercise':       ['gym', 'exercise equipment', 'weights', 'treadmill', 'yoga mat', 'ping pong table'],\n",
    "    'kitchen':        ['kitchen', 'appliance', 'refrigerator', 'oven', 'stove', 'fridge'],\n",
    "    'safety':         ['safety', 'security', 'alarm', 'detector', 'lock', 'first aid'],\n",
    "    'entertainment':  ['entertainment', 'tv', 'sound system', 'game console', 'books', 'board games'],\n",
    "    'outdoors':       ['outdoors', 'backyard', 'garden', 'patio', 'balcony', 'grill', 'bbq'],\n",
    "    'cooking_basics': ['cooking basics', 'dinnerware', 'utensils', 'cups', 'glasses', 'forks', 'spoons', 'pots']\n",
    "}\n",
    "\n",
    "def classify_single_amenity(raw_amenity, category_map, classifier, brand_remover):\n",
    "    raw_amenity_lower = raw_amenity.lower()\n",
    "    for category, search_terms in category_map.items():\n",
    "        for term in search_terms:\n",
    "            if re.search(r'\\b' + re.escape(term) + r'\\b', raw_amenity_lower):\n",
    "                return category\n",
    "    brand_free_text = brand_remover.remove_brands(raw_amenity)\n",
    "    cleaned_text = clean_amenity_text(brand_free_text)\n",
    "    return classifier.predict(cleaned_text)\n",
    "\n",
    "def inspect_amenity_classification(df, classifier, brand_remover):\n",
    "    all_unique_amenities = set(a for amenities_list in df['parsed_amenities'] for a in amenities_list)\n",
    "    print(f\"\\nFound {len(all_unique_amenities)} unique amenities to classify.\")\n",
    "    final_groups = defaultdict(list)\n",
    "    for raw_amenity in tqdm(sorted(list(all_unique_amenities)), desc=\"Classifying Amenities\"):\n",
    "        category = classify_single_amenity(raw_amenity, AMENITY_CATEGORIES, classifier, brand_remover)\n",
    "        if category:\n",
    "            final_groups[category].append(raw_amenity)\n",
    "        else:\n",
    "            final_groups['__UNCLASSIFIED__'].append(raw_amenity)\n",
    "    return final_groups\n",
    "\n",
    "def create_binary_columns(df, final_groups):\n",
    "    print(\"\\nCreating binary columns...\")\n",
    "    amenity_to_category_map = {}\n",
    "    for category, amenities in final_groups.items():\n",
    "        if category == '__UNCLASSIFIED__': continue\n",
    "        for amenity in amenities:\n",
    "            amenity_to_category_map[amenity] = category\n",
    "    all_categories = sorted([cat for cat in final_groups.keys() if cat != '__UNCLASSIFIED__'])\n",
    "    \n",
    "    tqdm.pandas(desc=\"Mapping Amenities to Categories\")\n",
    "    df['amenity_categories'] = df['parsed_amenities'].progress_apply(\n",
    "        lambda amenities_list: {amenity_to_category_map.get(a) for a in amenities_list if amenity_to_category_map.get(a)}\n",
    "    )\n",
    "    for category in tqdm(all_categories, desc=\"Creating Binary Columns\"):\n",
    "        column_name = f\"has_{category}\"\n",
    "        df[column_name] = df['amenity_categories'].apply(lambda cat_set: 1 if category in cat_set else 0)\n",
    "    df = df.drop(columns=['parsed_amenities', 'amenity_categories'])\n",
    "    print(\"✓ Binary columns created successfully.\")\n",
    "    return df\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == '__main__':\n",
    "    csv_file_path = r\"C:\\Users\\hodos\\Documents\\Uni\\Uni-Year-3\\Semester2\\Data\\cleaned_listings_amsterdam.csv\"\n",
    "    \n",
    "    # --- Initialization ---\n",
    "    classifier = AmenityClassifier(CATEGORY_REPRESENTATIVES)\n",
    "    brand_remover = FlairBrandRemover()\n",
    "    \n",
    "    print(f\"\\nLoading data from: {csv_file_path}\")\n",
    "    df = pd.read_csv(csv_file_path, engine='python', on_bad_lines='warn')\n",
    "    tqdm.pandas(desc=\"Parsing Amenities\")\n",
    "    df['parsed_amenities'] = df['amenities'].progress_apply(\n",
    "        lambda s: ast.literal_eval(s.strip()) if isinstance(s, str) and s.strip().startswith('[') else []\n",
    "    )\n",
    "\n",
    "    # --- Step 1: Inspect the classification ---\n",
    "    final_classified_groups = inspect_amenity_classification(df.copy(), classifier, brand_remover)\n",
    "    \n",
    "    print(\"\\n--- Amenity Classification Review ---\")\n",
    "    for category, amenities in sorted(final_classified_groups.items()):\n",
    "        print(f\"\\n--- Category: {category} ({len(amenities)} items) ---\")\n",
    "        for amenity in amenities[:100]:\n",
    "            print(f\"  - {amenity}\")\n",
    "        if len(amenities) > 100:\n",
    "            print(f\"  - ... and {len(amenities) - 100} more.\")\n",
    "\n",
    "    # --- Step 2: Create the final DataFrame with binary columns ---\n",
    "    transformed_df = create_binary_columns(df, final_classified_groups)\n",
    "\n",
    "    # --- Step 3: Display the final results ---\n",
    "    print(\"\\nTransformation complete. Here's a preview of the new columns:\")\n",
    "    amenity_cols = sorted([col for col in transformed_df.columns if col.startswith('has_')])\n",
    "    display_cols = ['id', 'name'] + amenity_cols\n",
    "    if len(display_cols) > 20:\n",
    "        print(f\"(Showing a subset of the {len(amenity_cols)} new amenity columns)\")\n",
    "        display_cols = display_cols[:20]\n",
    "    print(transformed_df[display_cols].head())\n",
    "\n",
    "    # --- Step 4: Save the new DataFrame to a CSV file ---\n",
    "    # Create the output filename dynamically\n",
    "    base_name = os.path.basename(csv_file_path)\n",
    "    name, ext = os.path.splitext(base_name)\n",
    "    output_filename = f\"{name}_with_amenity_cols{ext}\"\n",
    "    \n",
    "    print(f\"\\nSaving transformed DataFrame to: {output_filename}\")\n",
    "    # Use index=False to avoid writing the DataFrame index as a column\n",
    "    transformed_df.to_csv(output_filename, index=False)\n",
    "    print(\"✓ File saved successfully.\")"
   ],
   "id": "fb07c6a1430b237b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
